# version: "3.8"

# services:
#   api-gateway:
#     build:
#       context: .
#       dockerfile: ./api-gateway/Dockerfile
#       args:
#         - SERVICE_NAME=api-gateway
#     ports:
#       - "8080:8080"
#     env_file:
#       - ./api-gateway/.env
#     networks:
#       - default
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"

#   auth-service:
#     build:
#       context: .
#       dockerfile: ./auth-service/Dockerfile
#       args:
#         - SERVICE_NAME=auth-service
#     ports:
#       - "8081:8081"
#     env_file:
#       - ./auth-service/.env
#     depends_on:
#       - mongodb
#     networks:
#       - default
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"

#   product-service:
#     build:
#       context: .
#       dockerfile: ./product-service/Dockerfile
#       args:
#         - SERVICE_NAME=product-service
#     ports:
#       - "8082:8082"
#       - "8089:8089"
#     env_file:
#       - ./product-service/.env
#     environment:
#       - REDIS_URL=redis:6379
#     depends_on:
#       - mongodb
#       - kafka
#     networks:
#       - default
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"
      

#   cart-service:
#     build:
#       context: .
#       dockerfile: ./cart-service/Dockerfile
#       args:
#         - SERVICE_NAME=cart-service
#     ports:
#       - "8083:8083"
#       - "8090:8090"
#     env_file:
#       - ./cart-service/.env
#     depends_on:
#       - mongodb
#       - kafka
#     networks:
#       - default
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"      

#   order-service:
#     build:
#       context: . 
#       dockerfile: ./order-service/Dockerfile
#       args:
#         - SERVICE_NAME=order-service
#     ports:
#       - "8084:8084"
#     env_file:
#       - ./order-service/.env
#     depends_on:
#       - mongodb
#       - kafka
#     networks:
#       - default
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"
      

#   mongodb:
#     image: mongo:latest
#     ports:
#       - "27017:27017"
#     # environment:
#       # MONGO_INITDB_ROOT_USERNAME: root
#       # MONGO_INITDB_ROOT_PASSWORD: example
#     volumes:
#       - mongodb_data:/data/db
#     networks:
#       - default

#   mongo-express:
#     image: mongo-express:latest
#     ports:
#       - "8085:8081"
#     environment:
#       - ME_CONFIG_MONGODB_URL=mongodb://mongodb:27017/
#       - ME_CONFIG_MONGODB_SERVER=mongodb
#       - ME_CONFIG_MONGODB_PORT=27017
#       - ME_CONFIG_MONGODB_ENABLE_ADMIN=true
#       - ME_CONFIG_BASICAUTH_USERNAME=admin  # Set a username
#       - ME_CONFIG_BASICAUTH_PASSWORD=pass   # Set a password
#       - ME_CONFIG_MONGODB_NO_AUTH=true  
#     depends_on:
#       - mongodb
#     networks:
#       - default

#   elasticsearch:
#     image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
#     environment:
#       - discovery.type=single-node
#       - xpack.security.enabled=false
#     ulimits:
#       memlock:
#         soft: -1
#         hard: -1
#     volumes:
#       - elasticsearch_data:/usr/share/elasticsearch/data
#     ports:
#       - "9200:9200"
#     networks:
#       - default

#   search-service:
#     build:
#       context: .
#       dockerfile: ./search-service/Dockerfile
#     ports:
#       - "8086:8086"
#     env_file:
#       - ./search-service/.env
#     depends_on:
#       - kafka
#       - product-service
#       - elasticsearch
#     networks:
#       - default
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"
      
      
#   zookeeper:
#     image: confluentinc/cp-zookeeper:latest
#     container_name: zookeeper
#     ports:
#       - "2181:2181"
#     environment:
#       ZOOKEEPER_CLIENT_PORT: 2181
#       ZOOKEEPER_TICK_TIME: 2000
#     networks:
#       - default

#   kafka:
#     image: confluentinc/cp-kafka:latest
#     container_name: kafka
#     ports:
#       - "9092:9092"
#     environment:
#       KAFKA_BROKER_ID: 1
#       KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#       KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
#       KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#     depends_on:
#       - zookeeper
#     networks:
#       - default

#   kafka-setup:
#     image: confluentinc/cp-kafka:latest
#     container_name: kafka-setup
#     depends_on:
#       - kafka
#     command: >
#       bash -c "
#         echo -e 'Waiting for Kafka to be ready...' &&
#         cub kafka-ready -b kafka:9092 1 120 &&
#         echo -e 'Creating Kafka topics...' &&
#         kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --topic payment &&
#         kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --topic order_success &&
#         echo -e 'Successfully created the following topics:' &&
#         kafka-topics --list --bootstrap-server kafka:9092
#       "
#     networks:
#       - default

#   redis:
#     image: redis:latest
#     container_name: redis
#     ports:
#       - "6379:6379"
#     volumes:
#       - redis_data:/data
#     restart: always
#     networks:
#       - default

#   kibana:
#     image: docker.elastic.co/kibana/kibana:8.11.0
#     ports:
#       - "5601:5601"
#     environment:
#       ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"
#     depends_on:
#       - elasticsearch
#     networks:
#       - default

#   filebeat:
#     image: docker.elastic.co/beats/filebeat:8.11.0
#     container_name: fielbeat 
#     user: root 
#     volumes:
#       - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro # Mount file config filebeat.yml
#       - /var/lib/docker/containers:/var/lib/docker/containers:ro # Mount thư mục chứa log của các container
#       - /var/run/docker.sock:/var/run/docker.sock:ro # Mount Docker socket để Filebeat tự phát hiện container
#       - filebeatdata:/usr/share/filebeat/data # Lưu trạng thái của Filebeat
#     depends_on:
#       - elasticsearch
#     networks:
#       - default
#     command: filebeat -e -strict.perms=false 


# volumes:
#   mongodb_data:
#   redis_data:
#   elasticsearch_data:
#   filebeatdata:

# networks:
#   default:
#     driver: bridge



version: "3.8"

services:

  # PostgreSQL for review service
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_DB: reviewdb
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - default
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 3

  # API Gateway (giữ lại nếu cần backup hoặc migration)
  # api-gateway:
  #   build:
  #     context: .
  #     dockerfile: ./api-gateway/Dockerfile
  #     args:
  #       - SERVICE_NAME=api-gateway
  #   ports:
  #     - "8080:8080"
  #   env_file:
  #     - ./api-gateway/.env
  #   networks:
  #     - default
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "10m"
  #       max-file: "3"

  auth-service:
    build:
      context: .
      dockerfile: ./auth-service/Dockerfile
      args:
        - SERVICE_NAME=auth-service
    ports:
      - "8081:8081"
    env_file:
      - ./auth-service/.env
    environment:
      - MONGODB_URL=mongodb://mongodb:27017/authdb
      - JWT_SECRET=your-super-secret-jwt-key
    depends_on:
      - mongodb
    networks:
      - default
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  product-service:
    build:
      context: .
      dockerfile: ./product-service/Dockerfile
      args:
        - SERVICE_NAME=product-service
    ports:
      - "8082:8082"
      - "8089:8089"
    env_file:
      - ./product-service/.env
    environment:
      - REDIS_URL=redis:6379
      - MONGODB_URL=mongodb://mongodb:27017/productdb
    depends_on:
      - mongodb
      - kafka
      - redis
    networks:
      - default
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  cart-service:
    build:
      context: .
      dockerfile: ./cart-service/Dockerfile
      args:
        - SERVICE_NAME=cart-service
    ports:
      - "8083:8083"
      - "8090:8090"
    env_file:
      - ./cart-service/.env
    environment:
      - MONGODB_URL=mongodb://mongodb:27017/cartdb
    depends_on:
      - mongodb
      - kafka
    networks:
      - default
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  order-service:
    build:
      context: . 
      dockerfile: ./order-service/Dockerfile
      args:
        - SERVICE_NAME=order-service
    ports:
      - "8084:8084"
    env_file:
      - ./order-service/.env
    environment:
      - MONGODB_URL=mongodb://mongodb:27017/orderdb
    depends_on:
      - mongodb
      - kafka
    networks:
      - default
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  search-service:
    build:
      context: .
      dockerfile: ./search-service/Dockerfile
    ports:
      - "8086:8086"
    env_file:
      - ./search-service/.env
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - ELASTICSEARCH_INDEX=products
    depends_on:
      - kafka
      - elasticsearch
    networks:
      - default
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  mongodb:
    image: mongo:latest
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - default
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 30s
      timeout: 10s
      retries: 3

  mongo-express:
    image: mongo-express:latest
    container_name: mongo-express
    ports:
      - "8085:8081"
    environment:
      - ME_CONFIG_MONGODB_URL=mongodb://mongodb:27017/
      - ME_CONFIG_MONGODB_SERVER=mongodb
      - ME_CONFIG_MONGODB_PORT=27017
      - ME_CONFIG_MONGODB_ENABLE_ADMIN=true
      - ME_CONFIG_BASICAUTH_USERNAME=admin
      - ME_CONFIG_BASICAUTH_PASSWORD=pass
      - ME_CONFIG_MONGODB_NO_AUTH=true  
    depends_on:
      - mongodb
    networks:
      - default

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - default
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - default

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    networks:
      - default

  kafka-setup:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-setup
    depends_on:
      - kafka
    command: >
      bash -c "
        echo -e 'Waiting for Kafka to be ready...' &&
        cub kafka-ready -b kafka:9092 1 120 &&
        echo -e 'Creating Kafka topics...' &&        kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --topic payment &&
        kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --topic order_success &&
        kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --topic product_events &&
        kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --topic email_events &&
        echo -e 'Successfully created the following topics:' &&
        kafka-topics --list --bootstrap-server kafka:9092
      "
    networks:
      - default

  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: always
    networks:
      - default
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - default

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: filebeat
    user: root 
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - filebeatdata:/usr/share/filebeat/data
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - default
    command: filebeat -e -strict.perms=false

  

volumes:
  mongodb_data:
  redis_data:
  elasticsearch_data:
  filebeatdata:
  postgres_data:

networks:
  default:
    driver: bridge
